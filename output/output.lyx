#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble

\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement tph
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 25mm
\topmargin 25mm
\rightmargin 25mm
\bottommargin 25mm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset VSpace 3cm
\end_inset

National Research University Higher School of Economics
\begin_inset Newline newline
\end_inset

Faculty of Computer Science
\begin_inset Newline newline
\end_inset

Programme ’Master of Data Science’
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Date
MASTER’S THESIS
\begin_inset Newline newline
\end_inset

Evaluating risk bounds for AdaBoost on real datasets
\begin_inset VSpace 6.5cm
\end_inset


\end_layout

\begin_layout Address
Student: Krivodub Viacheslav Nikolaevich
\begin_inset Newline newline
\end_inset

Supervisor: Bruno Frederik Bauwens
\end_layout

\begin_layout Standard
\begin_inset VSpace 4cm
\end_inset


\begin_inset space \hspace{}
\length 70mm
\end_inset

Moscow, 2022
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Part*
Abstract
\end_layout

\begin_layout Standard
Current explanations for the success of AdaBoost relies on the margin theory
 in statistical learning theory, see 
\begin_inset CommandInset citation
LatexCommand cite
key "Freund1999"
literal "true"

\end_inset

.
 In this thesis we follow the explanations of the theory as given in Mohri's
 book
\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

.
 More specifically, a common interpretation of risk bounds states that classifie
rs with a larger 
\begin_inset Formula $L1$
\end_inset

-margin on the training set have a smaller generalization term, and hence
 are less likely to overfit the training data.
 However, this interpretation is not consistent with observations from real
 data as explained in 
\begin_inset CommandInset citation
LatexCommand cite
key "Breiman1997"
literal "true"

\end_inset

.
 In this paper, classifiers are trained that directly optimize the 
\begin_inset Formula $L1$
\end_inset

-margin, and they perform worse than AdaBoost on real datasets.
 Our first aim is to reproduce these experiments.
 We confirm that AdaBoost produce 
\begin_inset Formula $L1$
\end_inset

-margins that are smaller than the optimal ones.
 But then we observe the margin loss, which for AdaBoost is also smaller
 than for classifiers optimising 
\begin_inset Formula $L1$
\end_inset

-margin.
\end_layout

\begin_layout Standard
\noindent
The second goal of the thesis is to evaluate these risk bounds in detail,
 and inspect the contributions of the individual terms.
 For this we need to estimate Rademacher complexities and we explain how
 this can be done in practice.
 Both on artificial and real datasets we see that these risk bounds are
 very high compared to the error on test sets of the trained classifiers.
 In particular the generalization terms are too large.
 We conclude by pointing out some more complex recent theories that provide
 better risk bounds and might be the start point for further investigations.
\end_layout

\begin_layout Standard
\noindent
For our research we have created two versions of the AdaBoost algorithm,
 implementing the classical algorithm and optimising the 
\begin_inset Formula $L1$
\end_inset

-margin.
 Then, experiments were performed on artificial and real datasets of different
 types and sizes.
 The software code used in the experiments is available in 
\begin_inset CommandInset href
LatexCommand href
name "GitHub"
target "https://github.com/vkmbox/Ml-hse-final"
literal "false"

\end_inset

 repository.
\begin_inset Newpage newpage
\end_inset


\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
AdaBoost is short for Adaptive boosting and is a classification algorithm
 that proposes to benefit from linear combination of elementary classifiers
 to reduce generalisation error.
 At the same time, elementary classifiers can be simple or 
\begin_inset Quotes eld
\end_inset

weak
\begin_inset Quotes erd
\end_inset

 classifiers, for example decision stumps or hyperplanes into two classes
\begin_inset CommandInset citation
LatexCommand cite
key "Freund1999"
literal "true"

\end_inset

.
 This study investigates the task of 2-class classification on a real-value
 feature set.
 Suppose, there is a set of elementary 
\begin_inset Quotes eld
\end_inset

weak
\begin_inset Quotes erd
\end_inset

 classifiers:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
ℋ=\{h_{t}(x):t=1..N\}.\label{eq:hypothesis_set}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In other words, 
\begin_inset Formula $ℋ$
\end_inset

 is a hypothesis set.
 The resulting classifier is constructed as a sign-function of the elementary
 classifier ensemble:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x)=\mathrm{sign}(\sum{}_{k=1}^{T}\alpha_{k}h_{k}(x)),\alpha_{k}\ge0.\label{eq:adaboost_ensemble}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
The value T in the final ensemble is a number of boosting rounds and typically
 is much less than number N of classifiers.
 The scale of the coefficients 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
α
\begin_inset script subscript

\begin_layout Plain Layout
t
\end_layout

\end_inset

is insufficient for the result and in the following conclusions we consider
 their combinations satisfying the condition 
\begin_inset Formula $\sum{}_{k\text{=1}}^{N}α_{k}=1$
\end_inset

.
 Let the training set 
\begin_inset Formula $S_{m}=\{(x_{t,}y_{t})\},t=1..m$
\end_inset

, with 
\begin_inset Formula $(x_{t,}y_{t})∈X×\{-1,+1\}$
\end_inset

 and 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
a set of elementary classifiers 
\begin_inset Formula $ℋ$
\end_inset

 be given.
 Then, the learning algorithm's goal is to
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 find 
\begin_inset Formula ${α_{k}(x)}$
\end_inset

 such that an ensemble 
\begin_inset Formula $f(x)$
\end_inset

 has low generalisation error.
\end_layout

\begin_layout Standard
\noindent
Originally, AdaBoost was implemented as an iterative process, when in each
 iteration the boosting round selects some specific weak classifier that
 minimises the error on some specially designed distribution.
 Alternatively, the 
\begin_inset Formula $L1$
\end_inset

-geometric margin in the separable case for the training set can be defined
 and maximised explicitly, e.g.
 by using the linear programming method.
 Thus, we obtain two ideas for the classifier essemble implementation and
 moreover the second version is a margin maximiser.
 If it is true that maximising the margin yields a smaller generalisation
 error, the second version classifier should give better results.
 The following paragraphs discuss this statement in more detail.
\end_layout

\begin_layout Section
\noindent
Theoretical basis and implementation specifics
\begin_inset CommandInset label
LatexCommand label
name "sec:Theoretical-basis"

\end_inset


\end_layout

\begin_layout Subsection
\noindent
AdaBoost algorithm
\end_layout

\begin_layout Standard
\noindent
As mentioned in Introduction, original AdaBoost is an iterative algorithm
 which selects a week classifier on each boosting round and adds it to the
 resulting ensemble with the appropriate weight 
\begin_inset Formula $\alpha_{t}$
\end_inset

.
 Firstly, the hypothesis set for the studying case must be defined.
 As a weak classifier for the real-valued features input we consider decision
 stumps which are effectively a one-level decision tree classifiers.
 A decision stump has a threshold value and takes only one of the features
 for classification.
 It compares the input value with the threshold and returns one of two classes
 after comparison.
 Thus, for a feature with m different values in train set there are 2
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
m decision stumps which are distinguishable in terms of classification result.
 Finally, for a train set with n features and m samples there are at most
 2
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
n
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
m different weak classifiers and the hypothesis set 
\begin_inset Formula $ℋ$
\end_inset

 is finite.
 To build 
\begin_inset Formula $ℋ$
\end_inset

 for the given test set we should consider decision stumps for all features
 and in each feature - 2 different stamps for each feature's value.
\end_layout

\begin_layout Standard
\noindent
Further, the selection from the hypothesis set on each boosting round is
 performed.
 For this, the distribution 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\{D_{k}(t)\},t=1..m$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 is maintained, which is initially uniform.
 On each iteration the weak classifier is selected to minimize the error:
\begin_inset Formula 
\[
\underset{h\inℋ}{h_{k}=\mathrm{arg}\,\mathrm{min}}\sum{}_{t\text{=1}}^{m}D_{k}(t)[h(x_{t})≠y_{t}]\,.
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
The error of the selected classifier 
\begin_inset Formula $ɛ_{k}$
\end_inset

 must be less than 1/2, otherwise the algorithm stops.
 Then, the coefficient in composition is calculated: 
\begin_inset Formula 
\[
α_{k}=0.5·\mathrm{log}((1-ɛ_{k})/ɛ_{k}).
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
Thus, 
\begin_inset Formula $α_{k}$
\end_inset

 is positive and the less error 
\begin_inset Formula $ɛ_{k}$
\end_inset

, the bigger this coefficient.
 After that, the next distribution 
\begin_inset Formula $D_{k+1}$
\end_inset

is calculated such that elements with a wrong prediction of 
\begin_inset Formula $h_{k}$
\end_inset

have larger weight, while all others have smaller one:
\begin_inset Formula 
\[
D_{k+1}(t)=D_{k}(t)·\mathrm{exp}(-α_{k}·y_{t}·h_{k}(x_{t}))/Z_{t}.
\]

\end_inset

The value 
\begin_inset Formula $Z_{t}$
\end_inset

 is a normalization factor for the distribution 
\begin_inset Formula $D_{k+1}$
\end_inset

.
 Therefore, the algorithm on each iteration tries to get a weak classifier
 that minimises errors of previously selected ones.
 The empirical error of AdaBoost classifier decreases exponentially fast
 as a function of the number of boosting rounds 
\begin_inset Formula $T$
\end_inset

 in formula
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:adaboost_ensemble"
plural "false"
caps "false"
noprefix "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

.
 
\end_layout

\begin_layout Standard
\noindent
In this study the separable data sets are explored, more specifically, the
 training set is expected to be labeled by the final classifier without
 errors, i.e.
 
\begin_inset Formula $y_{t}∙f(x_{t})>0,t=1..m$
\end_inset

.
 For this case we can define 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- geometric
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 margin, which characterises a confidence of the classifier 
\begin_inset Formula $f$
\end_inset

 on the training data set:
\begin_inset Formula 
\begin{equation}
ρ_{f}=\underset{t=1..m}{\mathrm{min}}|α·h(x_{t})|/\|α\|_{1}.\label{eq:ro_f}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In other words, 
\begin_inset Formula $ρ_{f}$
\end_inset

 is a minimum among norm-1 distances from 
\begin_inset Formula $h(x_{t})$
\end_inset

 vectors to the hyperplane of equation 
\begin_inset Formula $α·x=0$
\end_inset

 in 
\begin_inset Formula $R^{T}$
\end_inset

.
 In the following paragraphs we will explore the geometric margin value
 and its relationship to other parameters in different experiments.
\end_layout

\begin_layout Standard
\noindent
For the purposes of this study, the algorithm described above is implemented
 as a 
\begin_inset CommandInset href
LatexCommand href
name "Java application"
target "https://github.com/vkmbox/Ml-hse-final/tree/master/ada-boost-implementations/code-java"
literal "false"

\end_inset

 with Python proxy-class 
\begin_inset CommandInset href
LatexCommand href
name "ada_boost_java_v1.py"
target "https://github.com/vkmbox/Ml-hse-final/blob/master/ada-boost-implementations/code-python/adaboost_java/ada_boost_java_v1.py"
literal "false"

\end_inset

, library py4j.java_gateway is used to make calls to the Java process from
 Python code.
 The advantage of this approach is its performance; when using Java code,
 the execution time is reduced several times.
 However, in the current implementation, due to the specificity of py4j.java_gate
way, only one instance of the classifier can be handled at a time, also,
 the java process must be started manually before calculations.
 This implementation uses ND4J library
\begin_inset CommandInset citation
LatexCommand cite
key "nd4j"
literal "true"

\end_inset

 on Java side which is functionally similar to Python's NumPy and SciPy.
 Among other things, this implementation can serve as an example of the
 recently improving support on the Java stack for data science computing.
\end_layout

\begin_layout Subsection
\noindent
Linear programming algorithm
\end_layout

\begin_layout Standard
\noindent
The 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- geometric
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 margin which is calculated on a trained AdaBoost classifier by formula
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ro_f"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be maximised explicitly.
 For this, in the separable case the linear programming problem can be formulate
d for the 
\begin_inset Formula $ρ$
\end_inset

 maximisation:
\begin_inset Formula 
\begin{align}
\underset{α}{\mathrm{\text{maximise}}}\,ρ,\label{eq:max_1}\\
\text{subject to}: & y_{t}(α·h(x_{t}))/\|α\|_{1}≥ρ,t=1..m.\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In this problem vectors 
\begin_inset Formula $α$
\end_inset

 and 
\begin_inset Formula $h(x_{t})$
\end_inset

 must have dimensionality equal to size of set 
\begin_inset Formula $ℋ$
\end_inset

.
 As described in paragraph 2.1, for training set with n features and m samples
 
\begin_inset Formula $ℋ$
\end_inset

 consists of at most 2
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
n
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
m classifiers.
 Also, in view of formula 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ro_f"
plural "false"
caps "false"
noprefix "false"

\end_inset

 it is reasonable to add the condition 
\begin_inset Formula $\|α\|_{1}=1$
\end_inset

 and get rid of the denominator in the constraint.
 As a variables of the task a scalar value 
\begin_inset Formula $ρ$
\end_inset

 and vector 
\begin_inset Formula $α$
\end_inset

 must be taken.
 Coordinates of 
\begin_inset Formula $α$
\end_inset

 should be non-negative for model unambiguity, because a base classifier
 with a negative coefficient is equivalent to the symmetric classifier with
 the same modulo positive coefficient.
 The parameter 
\begin_inset Formula $ρ$
\end_inset

 is expected to be positive as a result of optimisation in separable case.
\end_layout

\begin_layout Standard
\noindent
For this problem the implementation in Python with library scipy.optimize.linprog
 is used.
 However, this library does not allow variables in right sides of inequalities
 and accepts only minimisation problems.
 Thus, the original problem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:max_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 must be slightly transformed.
 Firstly, a vector of variables must be defined, let's denote it as 
\begin_inset Formula $u$
\end_inset

, with first coordinates coinsiding with 
\begin_inset Formula $α$
\end_inset

 and the latest coordinate equal to 
\begin_inset Formula $ρ$
\end_inset

, 2
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
n
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
m+1 coordinates in total.
 Let's denote 2
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
n
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
m as N.
 Then, the target function is as follows:
\begin_inset Formula 
\[
\underset{u}{\mathrm{min}}-u_{N+1}.
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
Next, there are m constraints for each sample from test set:
\begin_inset Formula 
\[
u_{1}(-y_{t}·h_{1}(x_{t}))+...+u_{N}(-y_{t}·h_{N}(x_{t}))+u_{N+1}≤0,t=1..m.
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
and constraint 
\begin_inset Formula $\|α\|_{1}=1$
\end_inset

:
\begin_inset Formula 
\[
\sum{}_{k\text{=1}}^{N}u_{k}=1.
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
On top of that, bounds for 
\begin_inset Formula $α$
\end_inset

 must be defined:
\begin_inset Formula 
\[
u_{k}≥0,k=1..N.
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
To formulate the task, all classifiers from 
\begin_inset Formula $ℋ$
\end_inset

 on all test samples must be calculated.
 In the current example for training set with n features and m samples there
 are 
\begin_inset Formula $N·m=2·n·m^{2}$
\end_inset

 values which is a big value in real experiments.
 Nevertheless, the linear algorithm directly maximises 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- geometric
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 margin for the given hypothesis set 
\begin_inset Formula $ℋ$
\end_inset

 and therefore is of theoretical interest.
\end_layout

\begin_layout Standard
\noindent
This version of the algorithm is implemented as a Python class 
\begin_inset CommandInset href
LatexCommand href
name "ada_boost_linear_v1.py"
target "https://github.com/vkmbox/Ml-hse-final/blob/master/ada-boost-implementations/code-python/ada_boost_linear_v1.py"
literal "false"

\end_inset

.
 It implements standard classifier method, also, 
\begin_inset Formula $L_{1}$
\end_inset

- geometric margin calculation is realised in this class.
\end_layout

\begin_layout Subsection
\noindent
Margin theory
\begin_inset CommandInset label
LatexCommand label
name "subsec:Margin-theory"

\end_inset


\end_layout

\begin_layout Standard
\noindent
In line with statistical learning theory, there are estimations of generalisatio
n error for real-valued hypothesis.
 At the beginning, some notions need to be defined.
 The following definitions are generally accepted in statistic learning
 theory and cited according to the book 
\begin_inset Quotes eld
\end_inset

Foundations of Machine Learning
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

.
\end_layout

\begin_layout Standard
\noindent

\emph on
Risk which is also called generalisation error.

\emph default
 For a given hypothesis 
\begin_inset Formula $h\inℋ$
\end_inset

 , true classifier 
\begin_inset Formula $c$
\end_inset

 and distribution 
\begin_inset Formula $𝒟$
\end_inset

, the generalisation error 
\begin_inset Formula $R$
\end_inset

 is defined as follows:
\begin_inset Formula 
\begin{equation}
R(h)=\underset{x\sim𝒟}{𝕇}[h(x)≠c(x)]=\underset{x\sim𝒟}{𝔼}[1_{h(x)≠c(x)}].\label{eq:gen_err}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Empirical risk or empirical error.

\emph default
 For a given hypothesis 
\begin_inset Formula $h\inℋ$
\end_inset

 , true classifier 
\begin_inset Formula $c$
\end_inset

 and a sample 
\begin_inset Formula $S_{m}=(x_{1},...,x_{m})$
\end_inset

, the empirical error or empirical risk of 
\begin_inset Formula $h$
\end_inset

 is defined by
\begin_inset Formula 
\begin{equation}
\hat{R}_{S}(h)=\frac{1}{m}\sum{}_{t\text{=1}}^{m}1_{h(x)≠c(x).}\label{eq:emp_err}
\end{equation}

\end_inset

Note, that 
\begin_inset Formula $𝔼\hat{R}_{S}(h)=R(h)$
\end_inset

 and 
\begin_inset Formula $\hat{R}_{S}(h)$
\end_inset

 is an unbiased estimate of 
\begin_inset Formula $R(h)$
\end_inset

.
 Also, since samples in 
\begin_inset Formula $S_{m}$
\end_inset

 are independent and the random variable 
\begin_inset Formula $1_{h(x)≠c(x)}$
\end_inset

 has finite expectation value, 
\begin_inset Formula $\hat{R}_{S}(h)$
\end_inset

 converges to 
\begin_inset Formula $R(h)$
\end_inset

 by measure 
\begin_inset Formula $𝕇$
\end_inset

 which can be proven for example by Khinchin theorem
\begin_inset CommandInset citation
LatexCommand cite
key "Borovkov1998"
literal "true"

\end_inset

.
 Consequently, 
\begin_inset Formula $\hat{R}_{S}(h)$
\end_inset

 is an unbiased and consistent estimator of 
\begin_inset Formula $R(h)$
\end_inset

, in subsequent experiments 
\begin_inset Formula $\hat{R}_{S}(h)$
\end_inset

 is used for 
\begin_inset Formula $R(h)$
\end_inset

 estimations.
\end_layout

\begin_layout Standard
\noindent

\emph on
Empirical Rademacher complexity for hypothesis set.

\emph default
 Let 
\begin_inset Formula $ℋ$
\end_inset

 be a family of functions taking values in {−1, +1}, then for sample 
\begin_inset Formula $S_{m}$
\end_inset


\begin_inset Formula 
\begin{equation}
\hat{𝔑_{s}}(ℋ)=\frac{1}{m}\underset{σ}{𝔼}[\underset{h\inℋ}{\mathrm{sup}}\sum{}_{t\text{=1}}^{m}σ_{t}·h(x_{t})].\label{eq:rad_h}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In the formula above 
\begin_inset Formula $σ=(σ_{1},...,σ_{m})$
\end_inset

, where 
\begin_inset Formula $σ_{t},t=1..m$
\end_inset

 are independent uniform random variables taking values in {−1, +1}.
\end_layout

\begin_layout Standard
\noindent

\emph on
Empirical margin loss
\emph default
 for sample 
\begin_inset Formula $S_{m}$
\end_inset

:
\begin_inset Formula 
\begin{equation}
\hat{R}_{S,ρ}(h)=\frac{1}{m}\sum{}_{t\text{=1}}^{m}𝛷_{ρ}(y_{t}·h(x_{t})).\label{eq:mg_loss}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In this formula 
\begin_inset Formula $𝛷_{ρ}(x)=\mathrm{min}(1,\mathrm{max}(0,1-x/ρ))$
\end_inset

, the graph of the function is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Ф_func"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename pic/loss_function.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Margin loss function
\begin_inset CommandInset label
LatexCommand label
name "fig:Ф_func"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Finally, the upper bound of the generalisation error is as follows.
 Let 
\begin_inset Formula $ℋ$
\end_inset

 be a family of real-valued functions and let the non-negative value 
\begin_inset Formula $ρ$
\end_inset

 of the margin be fixed.
 Then, for any 
\begin_inset Formula $δ>0$
\end_inset

, with probability at least 
\begin_inset Formula $1−δ$
\end_inset

, the following holds for all 
\begin_inset Formula $h∈ℋ$
\end_inset

:
\begin_inset Formula 
\begin{equation}
R(h)≤\hat{R}_{S,ρ}(h)+\frac{2}{ρ}\hat{𝔑_{s}}(ℋ)+3\sqrt{\frac{\mathrm{log}\frac{1}{δ}}{2m}}.\label{eq:est_1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Importantly, that this formula remains valid if we consider convex linear
 combinations of classifiers in 
\begin_inset Formula $ℋ$
\end_inset

 , i.
 e.
 if 
\begin_inset Formula $h(x)\in conv(ℋ)$
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

, where 
\begin_inset Formula $conv(ℋ)$
\end_inset

 is:
\begin_inset Formula 
\begin{equation}
conv(ℋ)=\{\sum{}_{t\text{=1}}^{m}µ_{t}h_{t}:m≥1,∀t=1..m,\text{µ}_{t}≥0,h_{t}\inℋ,\sum{}_{t\text{=1}}^{m}\text{µ}_{t}≤1\}.\label{eq:conv}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
As AdaBoost classifiers 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:adaboost_ensemble"
plural "false"
caps "false"
noprefix "false"

\end_inset

 of both previously described versions are ensembles satisfying condition
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:conv"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the hypothesis set of base classifiers can be used instead of original
 AdaBoost ensembles in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 , which simplifies calculation.
 Since both versions work with the same hypothesis set 
\begin_inset Formula $ℋ$
\end_inset

 and we may fix 
\begin_inset Formula $δ$
\end_inset

 value for all experiments, the difference between them depends on the margin
 loss term 
\begin_inset Formula $\hat{R}_{S,ρ}(h)$
\end_inset

 only.
 According to the theory, for margin loss calculation we use the final classifie
r's ensemble 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:adaboost_ensemble"
plural "false"
caps "false"
noprefix "false"

\end_inset

 but without an external sign operator.
 Thus, in separable case expression 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mg_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be rewritten as follows:
\begin_inset Formula 
\begin{equation}
\hat{R}_{S,ρ}(h)=\frac{1}{m}\sum{}_{t\text{=1}}^{m}𝛷_{ρ}(α·h(x_{t})).\label{eq:mg_loss_sep}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
A comparison of formulae 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ro_f"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mg_loss_sep"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows that for 
\begin_inset Formula $ρ<ρ_{f}$
\end_inset

 
\begin_inset Formula $\hat{R}_{S,ρ}(h)=0$
\end_inset

.
 Obviously, linear programming algorithm yields larger margin, but the question
 if this specificity is decisive in estimating 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is debatable and will be addressed in subsequent experiments.
\end_layout

\begin_layout Section
\noindent
Experiments
\end_layout

\begin_layout Standard
\noindent
In the previous paragraphs, several questions have been posed for investigation
 in real-data experiments.
 Firstly, there is a 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- geometric
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 margin 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ro_f"
plural "false"
caps "false"
noprefix "false"

\end_inset

 which reflects the classifier's confidence on train data set, and the question
 is whether greater 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- geometric
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 margin provides less generalisation error on test data.
 Secondly, we need to check terms of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 on real data and estimate the gap between the right-hand side of this inequalit
y and the errors in the test samples.
\end_layout

\begin_layout Subsection
Estimation of 
\begin_inset Formula $L_{1}$
\end_inset

- geometric margin
\begin_inset CommandInset label
LatexCommand label
name "subsec:estimation-l1"

\end_inset


\end_layout

\begin_layout Standard
Estimations for geometric margin have been performed on synthetic or generated
 and real data sets.
 Results are represented in 
\begin_inset CommandInset href
LatexCommand href
name "Geometric_Margin_Estimations.ipynb"
target "https://github.com/vkmbox/Ml-hse-final/blob/master/ada-boost-implementations/code-python/notebooks/Geometric_Margin_Estimations.ipynb"
literal "false"

\end_inset

 notebook.
 For data generation the Python function 
\begin_inset Newline newline
\end_inset

sklearn.datasets.make_classification was used with the following default parameter
s: number of samples=300, total number of features=50, number of informative
 features=20, number of redundant features=20, number of duplicated features=10,
 number of clusters per class=5.
 Since we plan to perform experiments on separable data sets, labels for
 samples X are artificially separated.
 For this, a random vector is generated and the labels {-1, +1} are created
 according to the projection of X onto this vector.
 Moreover, samples X are artificially extended from the hyper-plane perpendicula
r to the vector by a gap value, the default gap is 0.1.
 Varying the gap we may simulate different kinds of separable or non-separable
 data sets.
 Default splitting of generated data is 
\begin_inset Formula $\frac{2}{3}$
\end_inset

 for train set and 
\begin_inset Formula $\frac{1}{3}$
\end_inset

 for test set.
\end_layout

\begin_layout Subsubsection
Synthetic data sets with a varying number of properties
\end_layout

\begin_layout Standard
\noindent
In the first experiment, there are data sets on 300 rows with the number
 of features ranging from 25 to 500 in increments of 25.
 For each number of features, 5 data sets are generated and the results
 are averaged.
 Both version of classifiers (linear and AdaBoost) are learned on subset
 of 200 rows and then empirical risk is estimated on 100 rows set.
 The 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
margin value on the train set and the empirical risk on the test set in
 the experiments for both versions of the classifier is illustrated in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:300_features_varying"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pic/features_margin.png
	width 50col%

\end_inset


\begin_inset Graphics
	filename pic/features_risk.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Margin and empirical risk depending on features count in experiments with
 300 samples data sets
\begin_inset CommandInset label
LatexCommand label
name "fig:300_features_varying"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
As can be seen from the graphs, geometrical margin for the linear algorithm
 is higher, which is explained by the different principles of the algorithms.
 For AdaBoost the number of non-zero coefficients in the linear combination
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:adaboost_ensemble"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is limited by the quantity of boosting rounds, while in the linear algorithm
 version a coefficient of any weak classifier from hypothesis set 
\begin_inset Formula $ℋ$
\end_inset

 can be non-zero and set of possible combinations is obviously larger in
 the latter case.
 At the same time, the empirical risk on the test set does not differ significan
tly between the versions of the classifiers, as can be seen from the right-hand
 graph.
\end_layout

\begin_layout Standard
\noindent
Also, in this experiment we can investigate if there is a statically significant
 difference between empirical risks of AdaBoost and linear classifiers.
 For analysis we have two sets of risk values with the corresponding values
 referring to the results of the classifiers on the same data set.
 It is difficult to determine the type of distribution of risk values, however,
 it can be hypothesised that these values has equal median.
 In other words, the null hypothesis of no difference on median 
\begin_inset Formula $H_{0}$
\end_inset

 can be tested against the alternative hypothesis 
\begin_inset Formula $H_{1}$
\end_inset

 of a statistical significant difference for the two-sided criterion.
 Since the distribution of variables is unknown, we may apply two sample
 non-parametric methods such as the sign
\begin_inset CommandInset citation
LatexCommand cite
key "Sign"
literal "true"

\end_inset

 and Wilcoxon
\begin_inset CommandInset citation
LatexCommand cite
key "Wilcoxon"
literal "true"

\end_inset

 tests.
 For sign test p–value=0.33 and for Wilcoxon test p–value=0.29 
\begin_inset CommandInset href
LatexCommand href
name "(link)"
target "https://github.com/vkmbox/Ml-hse-final/blob/master/ada-boost-implementations/code-python/notebooks/Geometric_Margin_Estimations.ipynb"
literal "false"

\end_inset

 and hypothesis 
\begin_inset Formula $H_{0}$
\end_inset

 is not rejected.
 The criterion Mann–Whitney U is not applicable here since error samples
 are dependent.
\end_layout

\begin_layout Subsubsection
Synthetic data sets with a varying number of samples
\end_layout

\begin_layout Standard
\noindent
A second experiment is performed with default number of features=50 and
 varying number of samples from 100 to 500 with step 50.
 Results are on figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:300_samples_varying"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pic/samples_margin.png
	width 50col%

\end_inset


\begin_inset Graphics
	filename pic/samples_risk.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Margin and empirical risk depending on samples number, 50 features
\begin_inset CommandInset label
LatexCommand label
name "fig:300_samples_varying"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Again, the significant difference in margin values can be seen, which does
 not, however, affect the empirical risk values.
 In this experiment sign test gives criterion value = 9 and p–value=0.006.
 The sign criterion is calculated for linear error minus AdaBoost error,
 consequently, the positive criterion value indicates that the empirical
 error of the linear classifier is larger than the AdaBoost's one.
 Moreover, p–value allows to assume this at a significance level of 0.05.
 Wilcoxon test supports the hypothesis of medians inequality with a p-value=0.015.
\end_layout

\begin_layout Subsubsection
Synthetic data sets with a varying percent of informative features
\end_layout

\begin_layout Standard
\noindent
In the next experiment, the number of samples and features are fixed to
 300 and 100 respectively.
 With the fixed 10% of repeated features, the percent of informative features
 is varying from 10% to 90% and the percent of redundant features is varying
 from 80% to 0% respectively.
 Results are shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:300_percent_varying"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pic/informative_margin.png
	width 50col%

\end_inset


\begin_inset Graphics
	filename pic/informative_risk.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Margin and empirical risk depending on percent of informative features,
 300 samples and 100 features
\begin_inset CommandInset label
LatexCommand label
name "fig:300_percent_varying"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
The ratio of the data is similar to previous experiments, difference in
 margin doesn't lead to better empirical risk.
 For this experiments, p–value are as follows: sign test - 0.26, Wilcoxon
 - 0.49.
 In both tests the hypothesis 
\begin_inset Formula $H_{0}$
\end_inset

 of equality in empirical risk between classifiers is accepted.
\end_layout

\begin_layout Subsubsection
\noindent
Breast cancer data set
\begin_inset CommandInset label
LatexCommand label
name "subsec:breast-cancer-l1"

\end_inset


\end_layout

\begin_layout Standard
\noindent
Of particular interest is a study on real data sets.
 Several experiments with real data sets from the UCI repository are described
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Breiman1997"
literal "true"

\end_inset

, here some of these data sets are retested.
 The first date set is Breast cancer
\begin_inset CommandInset citation
LatexCommand cite
key "Breast_Cancer"
literal "true"

\end_inset

, this data set has 9 informative features and 699 rows.
 There are 458 rows for benign and 241 rows for malignant cases, the data
 can be considered broadly balanced.
 After training on the whole data set, both classifier correctly predicts
 classes for all samples, i.e.
 the set is separable.
 The experiment is performed as follows: the complete data set is splitted
 20 times at random into training and test samples, with 1/4 of the records
 for the test, for each split the 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
geometric margin value on the train set and the empirical error on the test
 set is calculated, results are in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:breast-cancer"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="right" valignment="top">
<column alignment="right" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter name
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Average value
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Standard deviation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for linear algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0623
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0134
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for ada boost algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0517
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0096
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Geometric margin for linear algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0381
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0105
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Geometric margin for ada boost algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0166
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0142
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:breast-cancer"

\end_inset

Empirical error and geometric margin in experiments with breast cancer data
 set
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
For two sets of errors, 20 measurements for each classifiers, we apply statistic
 criteria to check significant difference between classifiers similar to
 previous experiments.
 Sign test gives p–value = 0.049 and Wilcoxon - p–value = 0.0046, both tests
 reject hypotesis 
\begin_inset Formula $H_{0}$
\end_inset

 about error median equality at 0.05 significance level.
 Finally, since the hypotheses of equality are rejected for the two-sided
 criterion and the error results are higher for the linear algorithm, we
 can accept the hypothesis of a larger error for the linear algorithm at
 the 0.05 significance level.
\end_layout

\begin_layout Subsubsection
\noindent
Ionosphere data set
\begin_inset CommandInset label
LatexCommand label
name "subsec:Ionosphere-l1"

\end_inset


\end_layout

\begin_layout Standard
\noindent
The second real data set that is studied is Ionosphere
\begin_inset CommandInset citation
LatexCommand cite
key "Ionosphere"
literal "true"

\end_inset

, there are 351 rows and 34 features in it.
 The set includes 126 rows for good and 225 rows for bad cases, thus the
 data is quite balanced.
 Since the set is relatively small, 50 splitting are performed with 1/4
 of the records for the test with the calculation of 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
-
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 geometric margin and empirical error for each splitting.
 Results are collected in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Ionosphere"
plural "false"
caps "false"
noprefix "false"

\end_inset

:
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="right" valignment="top">
<column alignment="right" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter name
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Average value
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Standard deviation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for linear algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1020
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0280
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for ada boost algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0931
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0280
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Geometric margin for linear algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1097
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0050
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Geometric margin for ada boost algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0962
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0054
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Ionosphere"

\end_inset

Empirical error and geometric margin in experiments with breast cancer data
 set
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Statistical criteria for two sets of errors with 50 measurements for each
 classifiers give p–values as follows: sign test - 6.96e-05, Wilcoxon - pvalue=0.0
002.
 Thus, both criteria reject the hypothesis 
\begin_inset Formula $H_{0}$
\end_inset

 about error median equality with high significance.
\end_layout

\begin_layout Subsubsection
\noindent
Conclusions on the dependence of the empirical error from 
\begin_inset Formula $L_{1}$
\end_inset

- geometric margin
\begin_inset CommandInset label
LatexCommand label
name "subsec:conclusions_l1"

\end_inset


\end_layout

\begin_layout Standard
No experiment described above shows a smaller empirical error on the test
 set for a linear algorithm classifier in comparison with AdaBoost one,
 though it always has larger 
\begin_inset Formula $L_{1}$
\end_inset

- geometric margin value on the training set.
 Conversely, in experiments with real data sets we get a statistically significa
nt difference in favour of a smaller empirical error in the AdaBoost case.
 To summarise, in comparison of AdaBoost and linear algorithm classifiers
 the higher 
\begin_inset Formula $L_{1}$
\end_inset

- geometric margin value on the training set doesn't guarantee the lower
 error on the test set.
 Moreover, on the same data AdaBoost typically gives error less than a linear
 algorithm classifier.
\end_layout

\begin_layout Subsection
Estimation of generalisation error
\begin_inset CommandInset label
LatexCommand label
name "subsec:estimation-error"

\end_inset


\end_layout

\begin_layout Standard
This section discusses the generalisation error estimations derived from
 inequality 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 For analysis of this inequality, it is presented here again:
\begin_inset Formula 
\begin{equation}
R(h)≤\hat{R}_{S,ρ}(h)+\frac{2}{ρ}\hat{𝔑_{s}}(ℋ)+3\sqrt{\frac{\mathrm{log}\frac{1}{δ}}{2m}}.\label{eq:est_1_dub}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
The inequality holds for any classifier 
\begin_inset Formula $h\inℋ$
\end_inset

 where 
\begin_inset Formula $ℋ$
\end_inset

 is a family of real-valued functions, with probability at least 
\begin_inset Formula $1−δ$
\end_inset

.
 As it is already discussed in paragraph 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Theoretical-basis"
plural "false"
caps "false"
noprefix "false"

\end_inset

, both AdaBoost and linear algorithms work with the same hypothesis set
 
\begin_inset Formula $ℋ$
\end_inset

 and fixing 
\begin_inset Formula $δ=0.05$
\end_inset

 in subsequent experiments we ensure the equality of right-hand second and
 third terms between the algorithms.
 Thus, the difference in calculation between classifiers is only in the
 value of the Emperical margin loss 
\begin_inset Formula $\hat{R}$
\end_inset

, which is evaluated by formula 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mg_loss_sep"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 To obtain an estimate of 
\begin_inset Formula $R(h)$
\end_inset

 from inequality 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1_dub"
plural "false"
caps "false"
noprefix "false"

\end_inset

, it is useful to consider the right-hand side of the inequality as a function
 of 
\begin_inset Formula $ρ:$
\end_inset


\begin_inset Formula 
\begin{equation}
U(ρ)=\hat{R}_{S,ρ}(h)+\frac{2}{ρ}\hat{𝔑_{s}}(ℋ)+3\sqrt{\frac{\mathrm{log}\frac{1}{δ}}{2m}}.\label{eq:up_rho}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
It may be noted that 
\begin_inset Formula $U(ρ)$
\end_inset

, contains differently directed terms: 
\begin_inset Formula $\hat{R}_{S,ρ}(h)$
\end_inset

 is a growing function of 
\begin_inset Formula $ρ$
\end_inset

 while 
\begin_inset Formula $\frac{2}{ρ}\hat{𝔑_{s}}(ℋ)$
\end_inset

 is decreasing.
 Obviously, to estimate 
\begin_inset Formula $R(h)$
\end_inset

 the lowerest possible value of 
\begin_inset Formula $U(ρ)$
\end_inset

 is required.
 It is difficult to solve this optimising task analytically because of the
 undifferentiated 
\begin_inset Formula $\hat{R}_{S,ρ}$
\end_inset

 as function of 
\begin_inset Formula $ρ$
\end_inset

, thus in the following experiments the value 
\begin_inset Formula $U(ρ)$
\end_inset

 is estimating empirically by variating of variable 
\begin_inset Formula $ρ$
\end_inset

.
\end_layout

\begin_layout Subsubsection
\noindent
Rademacher empirical complexity calculation on real data set
\end_layout

\begin_layout Standard
Rademacher empirical complexity is calculated by formula 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:rad_h"
plural "false"
caps "false"
noprefix "false"

\end_inset

:
\begin_inset Formula 
\begin{equation}
\hat{𝔑_{s}}(ℋ)=\frac{1}{m}\underset{σ}{𝔼}[\underset{h\inℋ}{\mathrm{sup}}\sum{}_{t\text{=1}}^{m}σ_{t}·h(x_{t})].\label{eq:rad_empirical_2}
\end{equation}

\end_inset

In this formula 
\begin_inset Formula $σ=(σ_{1},...,σ_{m})$
\end_inset

, where 
\begin_inset Formula $σ_{t},t=1..m$
\end_inset

 are independent uniform random variables taking values in {−1, +1}.
 Full sample space has size 
\begin_inset Formula $2^{m}$
\end_inset

 which is a quite big number on real training sets.
 However the calculation can be performed on a subset of the full space
 with satisfactory accuracy, this can be proved with usage of Hoeffding’s
 inequality
\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

.
\end_layout

\begin_layout Standard
\noindent
Hoeffding’s inequality: Let 
\begin_inset Formula $X_{1},...,X_{n}$
\end_inset

 be independent random variables with 
\begin_inset Formula $X_{k}$
\end_inset

 taking values in 
\begin_inset Formula $[a_{k},b_{k}],k=1...n$
\end_inset

.
 Then, for any 
\begin_inset Formula $ε>0$
\end_inset

, the following inequalities hold for 
\begin_inset Formula $S_{n}=\sum{}_{k\text{=1}}^{n}X_{k}$
\end_inset

:
\begin_inset Formula 
\begin{align}
P[(S−E[S_{n}]≥ε)≤e^{-2ε^{2}/\sum{}_{k=1}^{n}(b_{k}-a_{k})^{2}},\label{eq:hoeffding-ineq}\\
P(S_{n}−E[S_{n}]≤-ε)≤e^{-2ε^{2}/\sum{}_{k=1}^{n}(b_{k}-a_{k})^{2}}.\nonumber 
\end{align}

\end_inset

Then, consider the space of events 
\begin_inset Formula $Ω=\{σ=(σ_{1},...,σ_{m})\},P(σ)=\frac{1}{2^{m}}$
\end_inset

, and random variable 
\begin_inset Formula $X_{k}(σ)=\underset{h\inℋ}{\frac{1}{m}\mathrm{sup}}\sum{}_{t\text{=1}}^{m}σ_{t}·h(x_{t})$
\end_inset

.
 In this case 
\begin_inset Formula $X_{k}(σ)\in[-1,1]$
\end_inset

.
 Also, if 
\begin_inset Formula $σ_{k}$
\end_inset

 is selected from 
\begin_inset Formula $Ω$
\end_inset

 with repetition, then 
\begin_inset Formula $X_{k}=X(σ_{k})$
\end_inset

 are independent for different k since different measurements do not affect
 each other.
 Thus, 
\begin_inset Formula $X_{k}$
\end_inset

 variables are i.i.d.
 Finally, we may use 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hoeffding-ineq"
plural "false"
caps "false"
noprefix "false"

\end_inset

 as follows:
\begin_inset Formula 
\[
P(S_{n}−E[S_{n}]≥nε)=P(S_{n}/n−E[S_{n}]/n≥ε)≤e^{-2ε^{2}n^{2}/4n}=e^{-nε^{2}/2}.
\]

\end_inset

Notice, that due to linearity of the mathematical expectation and by definition
 of Rademacher empirical complexity 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:rad_empirical_2"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $E[S_{n}]/n=E[X_{k}(σ)]=\hat{𝔑_{s}}(ℋ)$
\end_inset

.
 Considering the symmetry of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hoeffding-ineq"
plural "false"
caps "false"
noprefix "false"

\end_inset

 with respect to 
\begin_inset Formula $E[S_{n}]$
\end_inset

, it can be concluded that
\begin_inset Formula 
\[
P(|S_{n}/n−\hat{𝔑_{s}}(ℋ)|≥ε)≤2e^{-nε^{2}/2}.
\]

\end_inset

Setting the level of significance 
\begin_inset Formula $δ$
\end_inset

, we obtain the following upper estimate of the value 
\begin_inset Formula $P(|S_{n}/n−\hat{𝔑_{s}}(ℋ)|≥ε)$
\end_inset

:
\begin_inset Formula 
\[
2e^{-nε^{2}/2}≤δ;n≥\frac{2}{ε^{2}}\mathrm{ln}\frac{2}{δ}.
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
For 
\begin_inset Formula $ε=0.1$
\end_inset

 and 
\begin_inset Formula $δ=0.05$
\end_inset

 we obtain 
\begin_inset Formula $n≥737.776$
\end_inset

, thus, substituting 
\begin_inset Formula $\hat{𝔑_{s}}(ℋ)$
\end_inset

 with 
\begin_inset Formula $S_{n}/n$
\end_inset

 on 1000 randomly selected Rademacher vectors should give a deviation within
 
\begin_inset Formula $±0.1$
\end_inset

 with 
\begin_inset Formula $P≥0.95$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Experiment on synthetic normally distributed data with 2 features and 3000
 samples
\begin_inset CommandInset label
LatexCommand label
name "subsec:3000_2"

\end_inset


\end_layout

\begin_layout Standard
First experiment is performed on synthetic data sampled from bi-variate
 normal distribution.
 3000 samples from the distribution provide features data set X.
 Then, similarly to experiments in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:estimation-l1"
plural "false"
caps "false"
noprefix "false"

\end_inset

, a random vector is generated and the labels {-1, +1} are created according
 to the projection of X onto this vector.
 Also, rows from X are artificially extended from the hyper-plane perpendicular
 to the generated vector by a gap value with the default gap 0.1.
 Results of this and subsequent experiments of paragraph 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:estimation-error"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are represented by 
\begin_inset CommandInset href
LatexCommand href
name "Rademacher_complexity_based_estimations.ipynb"
target "https://github.com/vkmbox/Ml-hse-final/blob/master/ada-boost-implementations/code-python/notebooks/Rademacher_complexity_based_estimations.ipynb"
literal "false"

\end_inset

 notebook.
\end_layout

\begin_layout Standard
\noindent
To find the value of the upper estimate of the generalisation error 
\begin_inset Formula $U(ρ)$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:up_rho"
plural "false"
caps "false"
noprefix "false"

\end_inset

, a data set is generated and split into training and test samples.
 Then, varying value 
\begin_inset Formula $ρ$
\end_inset

 from 0 to 1 with step 0.05, the value of 
\begin_inset Formula $U(ρ)$
\end_inset

 is calculated.
 For clarity, the graphs show the values of functions 
\begin_inset Formula $U(ρ)$
\end_inset

 and separately empirical margin loss 
\begin_inset Formula $\hat{R}_{S,ρ}$
\end_inset

 since the latter actually determines the difference in estimation between
 the versions of the classifier.
 For the experiment the data set with 2 features and 3000 samples is generated,
 then five different splittings are performed with test size equal to 
\begin_inset Formula $\frac{1}{3}$
\end_inset

 of all rows and results are averaged.
 The results of the experiment are shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:3000_2"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pic/3000-2-sum.png
	width 50col%

\end_inset


\begin_inset Graphics
	filename pic/3000-2-margin.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Generalisation error estimation and empirical margin loss on synthetic data
 sets with 2 features and 3000 samples
\begin_inset CommandInset label
LatexCommand label
name "fig:3000_2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Also, the following values for empirical risk are obtained (table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:3000_2"
plural "false"
caps "false"
noprefix "false"

\end_inset

):
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="3">
<features tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="right" valignment="top">
<column alignment="right" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter name
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Average value
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Standard deviation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for linear algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0054
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0030066593
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for AdaBoost algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0036
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0012000000
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Empirical error in experiments on synthetic data sets with 2 features and
 3000 samples
\begin_inset CommandInset label
LatexCommand label
name "tab:3000_2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
It can be seen from results that generalisation error estimations 
\begin_inset Formula $U(ρ)$
\end_inset

 for AdaBoost and linear classifiers are much larger than their empirical
 errors: the estimation of the generalisation error does not take values
 below 0.5, while the empirical error of the classifiers is 0.0036 and 0.0054
 in average.
 Also, it is noticeable that despite the fact that linear algorithm ensures
 larger 
\begin_inset Formula $L_{1}$
\end_inset

- geometric margin, it gives a consistently higher value for empirical margin
 loss as can be observed from the right-hand graph in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:3000_2"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 To sum up, according to experimental data, the expression of the right-hand
 side of the inequality 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1_dub"
plural "false"
caps "false"
noprefix "false"

\end_inset

 as a function of 
\begin_inset Formula $ρ$
\end_inset

 has a pronounced minimum on the interval (0,1), however, the value of this
 minimum is approximately 10 times higher than the empirical error values
 obtained in the experiments.
 Thus, we cannot rely on an estimate from this inequality in the current
 experiment.
\end_layout

\begin_layout Subsubsection
\noindent
Experiment on synthetic normally distributed data with 2 features and 10000
 samples
\end_layout

\begin_layout Standard
It can be supposed that the overestimation of generalisation error in the
 previous experiment can be explained by the insufficient number of samples
 in the training set.
 This experiment repeats the previous one on a larger data set of 10000
 samples.
 Thus, the data set with 2 features and 10000 samples is generated, then
 three different splittings are performed and results are averaged, all
 other conditions are the same as in the previous experiment.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:10000_2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the results.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pic/10000-2-sum.png
	width 50col%

\end_inset


\begin_inset Graphics
	filename pic/10000-2-margin.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Generalisation error estimation and empirical margin loss on synthetic data
 sets with 2 features and 10000 samples
\begin_inset CommandInset label
LatexCommand label
name "fig:10000_2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
The table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:10000_2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the values of the empirical error obtained in the experiment.
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="3">
<features tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="right" valignment="top">
<column alignment="right" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter name
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Average value
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Standard deviation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for linear algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0013997201
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0009271764
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for AdaBoost algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0011997600
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0006479445
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Empirical error in experiments on synthetic data sets with 2 features and
 10000 samples
\begin_inset CommandInset label
LatexCommand label
name "tab:10000_2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

It can be observed from the graphics and the table that results are mostly
 similar to ones from the previous experiment.
 Now, the generalisation error estimate has decreased to approximately value
 0.4 but is still several times larger than the empirical error from the
 experiments.
 From the comparison of the results of this and previous experiments, it
 can be concluded that as the number of samples in the training set increases,
 the estimate of generalisation error given by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1_dub"
plural "false"
caps "false"
noprefix "false"

\end_inset

 will be more accurate.
 From this perspective, the generalisation error estimation for the linear
 programming algorithm is expected to be higher (see right-hand graph on
 figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:10000_2"
plural "false"
caps "false"
noprefix "false"

\end_inset

) which is consistent with the values of the empirical error obtained in
 the experiments.
\end_layout

\begin_layout Subsubsection
Experiment on synthetic data with 5 features and 3000 samples
\end_layout

\begin_layout Standard
The previous two experiments were performed on data sets with a simple structure
 taken from a normal distribution, with no repeating or redundant features.
 This time the error estimation is checked on data X generated by the Python
 function sklearn.datasets.make_classification with parameters: total number
 of features=5, number of informative features=2, number of redundant features=2
, number of duplicated features=1, number of clusters per class=2.
 To obtain a separable data set, labels for samples X are created similarly
 to the previous cases according to the projection of X onto a randomly
 generated vector.
 The purpose of this experiment is to check if additional complexity of
 data such as redundant an duplicated features affects the result of error
 evaluation.
 Similarly to experiment in paragraph 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:3000_2"
plural "false"
caps "false"
noprefix "false"

\end_inset

, data with 3000 samples is generated and using three different splitting
 the generalisation error estimation and empirical margin loss is calculated,
 then results are averaged.
 Results of the experiment are on figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:3000_5"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pic/3000-5-sum.png
	width 50col%

\end_inset


\begin_inset Graphics
	filename pic/3000-5-margin.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Generalisation error estimation and empirical margin loss on synthetic data
 sets with 5 features and 3000 samples
\begin_inset CommandInset label
LatexCommand label
name "fig:3000_5"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Also, empirical errors obtained in the experiments are in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:3000_5"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="3">
<features tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="right" valignment="top">
<column alignment="right" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter name
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Average value
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Standard deviation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for linear algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0026666667
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0012472191
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for ada boost algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0033333333
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0012472191
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Empirical error in experiments on synthetic datasets with 5 features and
 3000 samples
\begin_inset CommandInset label
LatexCommand label
name "tab:3000_5"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
In comparison with the analogous experiment with 2 features it is can be
 observed that the minimum on the generalisation error estimation curve
 is now less pronounced and the estimation itself has deteriorated significantly
 to around 0.8.
 Thus, we may conclude that estimation based on the inequality 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1_dub"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is not robust to an increase in the amount of redundant data in the training
 sample.
\end_layout

\begin_layout Subsubsection
\noindent
Experiments on real datasets
\end_layout

\begin_layout Standard
\noindent
Finally, the inequality 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1_dub"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is checked on real data sets.
 First data set is Ionosphere
\begin_inset CommandInset citation
LatexCommand cite
key "Ionosphere"
literal "true"

\end_inset

, this data set was already used in paragraph 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Ionosphere-l1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for estimation of 
\begin_inset Formula $L_{1}$
\end_inset

- geometric margin.
 For this data 10 sequentual splittings are created with test size equal
 to 
\begin_inset Formula $\frac{1}{4}$
\end_inset

 of all rows and results are averaged.
 As usually, we calculate generalisation error estimation and empirical
 margin loss for 
\begin_inset Formula $ρ$
\end_inset

 from 0 to 1 with step 0.05.
 Results are presented on figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:iono_risk"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pic/iono-sum.png
	width 50col%

\end_inset


\begin_inset Graphics
	filename pic/iono-margin.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Generalisation error estimation and empirical margin loss on Ionosphere
 data set
\begin_inset CommandInset label
LatexCommand label
name "fig:iono_risk"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
As the right-hand graph shows, the generalisation error estimation curve
 does not even reach a value of 1 and thus this estimation is not suitable
 for practical application.
 The same situation applies to the Breast cancer dataset
\begin_inset CommandInset citation
LatexCommand cite
key "Breast_Cancer"
literal "true"

\end_inset

 already explored in paragraph 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:breast-cancer-l1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In a similar experiment to the previous one, the obtained results are shown
 in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:breast_cancer_risk"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pic/breast-sum.png
	width 50col%

\end_inset


\begin_inset Graphics
	filename pic/breast-margin.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Generalisation error estimation and empirical margin loss on Breast cancer
 dataset
\begin_inset CommandInset label
LatexCommand label
name "fig:breast_cancer_risk"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

From experiments on real datasets, it follows that the estimate 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1_dub"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for these cases is greater than 1.0 and therefore not applicable.
\end_layout

\begin_layout Subsubsection
Conclusions on the section
\begin_inset CommandInset label
LatexCommand label
name "subsec:сonclusion_error"

\end_inset


\end_layout

\begin_layout Standard
As the experiments have shown, the generalised error estimate based on inequalit
y 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1_dub"
plural "false"
caps "false"
noprefix "false"

\end_inset

 gives overestimated values not applicable in practice.
 As the samples number in the data set increases to several tens of thousands,
 the estimate decreases to 0.5 and below, but remains several times higher
 than the empirical error.
 In addition, the experiment with redundant and duplicate attributes shows
 that the estimate is not robust to increasing data complexity.
\end_layout

\begin_layout Section
\noindent
CONCLUSIONS
\end_layout

\begin_layout Standard
\noindent
Experiments with real data show that generalisation error estimation approaches
 based on 
\begin_inset Formula $L_{1}$
\end_inset

- geometric margin and inequality 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 do not demonstrate simple patterns.
 The study compares the classical AdaBoost algorithm with a linear programming
 version, which explicitly maximises geometric margin for the standard AdaBoost
 ensemble of base classifiers 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:adaboost_ensemble"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 However, AdaBoost demonstrates mostly better empirical error on synthetic
 and real data sets as shown in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:conclusions_l1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Moreover, AdaBoost provides better empirical margin loss values, although
 an estimate of generalisation error based on margin loss is overestimated
 and not practically applicable.
 The next conclusion of this study is regarding the minimum of 
\begin_inset Formula $U(ρ)$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:up_rho"
plural "false"
caps "false"
noprefix "false"

\end_inset

, which is an upper boundary of generalisation error estimation according
 to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 All experiments show that the minimum of 
\begin_inset Formula $U(ρ)$
\end_inset

 is several time larger than empirical error of both classifiers as as described
 in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:сonclusion_error"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 These facts motivate further research on the topic, the possible directions
 of which are described in the following paragraph.
\end_layout

\begin_layout Subsection
Future work
\end_layout

\begin_layout Standard
\noindent
Unsatisfactory generalisation error estimation derived from inequality 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 raises the question of how to improve it.
 Firstly, there are several intermediate inequation which are obtained during
 the prove of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

, one of them is formulated as follows
\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

:
\begin_inset Formula 
\begin{align}
R(h)≤\hat{R}_{S,ρ}(h)+2𝔑_{m}(ℋ^{*})+\sqrt{\frac{\mathrm{log}\frac{1}{δ}}{2m},}\\
ℋ^{*}=\{z=(x,y)⟼𝛷_{ρ}(y·h(x)):h\inℋ).\label{eq:H_hat}
\end{align}

\end_inset

In this formula 
\begin_inset Formula $𝛷_{ρ}(x)$
\end_inset

 is a margin loss function with graph in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Ф_func"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Further, to obtain the final 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 from this, the Talagrand’s lemma is applied
\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

, according to which the following relationship holds: 
\begin_inset Formula $𝔑_{m}(ℋ^{*})≤\frac{1}{ρ}𝔑_{m}(ℋ),h\inℋ$
\end_inset

.
 Recall from paragraph 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Margin-theory"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that for 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 it is possible to use the hypothesis set of base classifiers instead of
 original AdaBoost ensembles when the 
\begin_inset Formula $\hat{𝔑_{s}}(ℋ)$
\end_inset

 is calculated.
 However, this possibility has not been proven for the composition 
\begin_inset Formula $𝛷_{ρ}(y·h(x))$
\end_inset

, where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $h$
\end_inset

 is an 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
AdaBoost ensemble, and similar substitution cannot be used for hypothesis
 set 
\begin_inset Formula $ℋ^{*}$
\end_inset

.
 Calculating of 
\begin_inset Formula $\hat{𝔑_{s}}(ℋ^{*})$
\end_inset

 could be performed directly, however, it requires to solve the maximisation
 problem 
\begin_inset Formula $𝛷_{ρ}(\sigma·y·h(x)),h\inℋ$
\end_inset

 for each Rademacher vector 
\begin_inset Formula $σ$
\end_inset

, which is not obvious on infinite set 
\begin_inset Formula $ℋ$
\end_inset

 of AdaBoost ensembles.
\end_layout

\begin_layout Standard
\noindent
Besides, new publications on the topic of margin explanation have been appearing
 recently, for example article 
\begin_inset Quotes cld
\end_inset

On the doubt about margin explanation of boosting
\begin_inset Quotes frd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Doubt2013"
literal "true"

\end_inset

.
 In this master thesis a 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- geometric
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 margin 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ro_f"
plural "false"
caps "false"
noprefix "false"

\end_inset

 was investigated which is actually a minimum among norm-1 distances from
 
\begin_inset Formula $h(x_{t})$
\end_inset

 vectors to the hyper-plane of equation 
\begin_inset Formula $α·x=0$
\end_inset

.
 However, we observed that larger margin of linear algorithm does not ensure
 a lower value of error and margin loss, which would suggest that AdaBoost
 version generates better margins 
\begin_inset Quotes cld
\end_inset

in average
\begin_inset Quotes frd
\end_inset

.
 In the article the idea of whole margin distribution is studied in connection
 with average margin and variance.
 Also, there are some promising empirical error estimations in this work,
 for example theorem 10 proposes the bound decreasing proportionately 
\begin_inset Formula $\frac{1}{m}$
\end_inset

 with m equal to number of samples in training set.
 This theory implies hypothesis set with finite VC-dimension, thus we need
 to accurately calculate this value for the considering case.
\begin_inset Newpage newpage
\end_inset


\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "content"
options "unsrt"

\end_inset


\end_layout

\end_body
\end_document
