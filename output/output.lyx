#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement tph
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 25mm
\topmargin 25mm
\rightmargin 25mm
\bottommargin 25mm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset VSpace 3cm
\end_inset

National Research University Higher School of Economics
\begin_inset Newline newline
\end_inset

Faculty of Computer Science
\begin_inset Newline newline
\end_inset

Programme ’Master of Data Science’
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Date
MASTER’S THESIS
\begin_inset Newline newline
\end_inset

Evaluating risk bounds for AdaBoost on real datasets
\begin_inset VSpace 6.5cm
\end_inset


\end_layout

\begin_layout Address
Student: Krivodub Viacheslav Nikolaevich
\begin_inset Newline newline
\end_inset

Supervisor: Bruno Frederik Bauwens
\end_layout

\begin_layout Standard
\begin_inset VSpace 4cm
\end_inset


\begin_inset space \hspace{}
\length 70mm
\end_inset

Moscow, 2022
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Part*
Abstract
\end_layout

\begin_layout Standard
In line with current thinking on AdaBoost classifier explanation
\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

, the estimation of the classifier error relies on the margin theory.
 More specifically, classifiers with larger margin on training set are expected
 to work with less generalisation error.
 However, this is not always confirmed by experiments on real data
\begin_inset CommandInset citation
LatexCommand cite
key "Breiman1997"
literal "true"

\end_inset

.
 In addition, a theory for estimating the generalization error based on
 marginal loss and Radamacher complexity has been developed in the form
 of appropriate inequalities.
 The purpose of this thesis is to investigate the error estimation inequalities
 on real datasets in order to verify the estimates given by these inequalities
 and to try to explain the above contradiction.
 For this, two versions of the AdaBoost algorithm were implemented and experimen
ts were performed on datasets of different types and sizes.
 The software code used in the experiments is available in 
\begin_inset CommandInset href
LatexCommand href
name "GitHub"
target "https://github.com/vkmbox/Ml-hse-final"
literal "false"

\end_inset

 repository.
\begin_inset Newpage newpage
\end_inset


\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
AdaBoost, short for Adaptive boosting, is an classification algorithm that
 proposes to benefit from linear combination of elementary classifiers to
 reduce generalisation error.
 At the same time, elementary classifiers can be simple or 
\begin_inset Quotes eld
\end_inset

weak
\begin_inset Quotes erd
\end_inset

 classifiers, for example decision stumps or hyperplanes into two classes
\begin_inset CommandInset citation
LatexCommand cite
key "Freund1999"
literal "true"

\end_inset

.
 This study investigates the task of 2-class classification on a real-values
 features set.
 Suppose, there is a set of elementary 
\begin_inset Quotes eld
\end_inset

weak
\begin_inset Quotes erd
\end_inset

 classifiers:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
ℋ={h_{t}(x)},t=1..N\label{eq:hypothesis_set}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In other words, 
\begin_inset Formula $ℋ$
\end_inset

 is a hypothesis set.
 The resulting classifier is constructed as a sign-function of the elementary
 classifier ensemble:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x)=sign(\sum{}_{k=1}^{T}\alpha_{k}h_{k}(x)),\alpha_{k}\ge0\label{eq:adaboost_ensemble}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
The value T in the final ensemble is a number of boosting rounds and typically
 is much less than number N of classifiers.
 The scale of the coefficients 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
α
\begin_inset script subscript

\begin_layout Plain Layout
t
\end_layout

\end_inset

is insufficient for the result and in the following conclusions we consider
 their combinations satisfying the condition 
\begin_inset Formula $\sum{}_{k\text{=1}}^{N}α_{k}=1$
\end_inset

.
 Let the training set 
\begin_inset Formula $S_{m}=\{(x_{t,}y_{t})\},t=1..m$
\end_inset

, with 
\begin_inset Formula $(x_{t,}y_{t})∈X×{−1,+1}$
\end_inset

 and 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
a set of elementary classifiers 
\begin_inset Formula $ℋ$
\end_inset

 be given.
 Then, the learning algorithm's goal is to
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 find 
\begin_inset Formula ${α_{k}(x)}$
\end_inset

 such that an ensemble 
\begin_inset Formula $f(x)$
\end_inset

 has low generalisation error.
\end_layout

\begin_layout Standard
\noindent
Originally, AdaBoost was implemented as an iterative process, when in each
 interation the boosting round selects some specific weak classifier to
 minimize the resulting error.
 Apart from that, the margin in the separable case for the training set
 can be defined and maximised explicitly, e.g.
 by using the lnear programming method.
 Thus, we obtain two ideas for the classifier essemble implementation and
 moreover the second version is a margin maximiser.
 If it is true that maximising the margin yields a smaller generalisation
 error, the second version classifier should give better results.
 The following paragraphs discuss this statement in more detail.
\end_layout

\begin_layout Section
\noindent
Theoretical basis and implementation specifics
\begin_inset CommandInset label
LatexCommand label
name "sec:Theoretical-basis"

\end_inset


\end_layout

\begin_layout Subsection
\noindent
AdaBoost algorithm
\end_layout

\begin_layout Standard
\noindent
As it is mentioned in Introduction, original AdaBoost is an iterative algorithm
 which selects a week classifier on each boosting round and adds it to the
 resulting essemble with the appropriate weight 
\begin_inset Formula $\alpha_{t}$
\end_inset

.
 Firstly, the hypothesis set for the studying case must be defined.
 As a weak classifier for the real-values features input we consider decision
 stump which is efffectively a one-level decision tree clasifier.
 A decision stamp has a threshold value and takes only one of the features
 for classification.
 It compares the input value with the threshold and returns one of two classes
 after comparison.
 Thus, for a feature with m different values in train set there are 2
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
m decision stumps which are distinguishable in terms of classification result.
 Finally, for a train set with n features and m samples there are 2
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
n
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
m different weak classifiers and the hypothesis set 
\begin_inset Formula $ℋ$
\end_inset

 is finite.
 To build 
\begin_inset Formula $ℋ$
\end_inset

 for the given test set we should consider decision stumps for all features
 and in each feature - 2 different stamps for each feature's value.
\end_layout

\begin_layout Standard
\noindent
Further, the selection from the hypothesis set on each boosting round is
 performed.
 For this, the distribution 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\{D_{k}(t)\},t=1..m$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 is maintained, which is initially uniform.
 On each iteration the weak classifier is selected to minimize the error:
\begin_inset Formula 
\[
\underset{h\inℋ}{h_{k}=arg\,min}\sum{}_{t\text{=1}}^{m}D_{k}(t)[h(x_{t})≠y_{t}]\,
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
The error of the selected classifier 
\begin_inset Formula $ɛ_{k}$
\end_inset

 must be less than 1/2, otherwise the algorithm stops.
 Then, the coefficient in composition is calculated: 
\begin_inset Formula 
\[
α_{k}=0.5log((1-ɛ_{k})/ɛ_{k})
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
Thus, 
\begin_inset Formula $α_{k}$
\end_inset

 is positive and the less error 
\begin_inset Formula $ɛ_{k}$
\end_inset

, the bigger this coefficient.
 Afrer that, next distribution 
\begin_inset Formula $D_{k+1}$
\end_inset

is being calculated such that elements in position with wrong prediction
 of 
\begin_inset Formula $h_{k}$
\end_inset

are increasing, while all other - decreasing:
\begin_inset Formula 
\[
D_{k+1}(t)=D_{k}(t)·exp(-α_{k}·y_{t}·h_{k}(x_{t}))/Z_{t}
\]

\end_inset

Value 
\begin_inset Formula $Z_{t}$
\end_inset

 is a normalization factor for the distribution 
\begin_inset Formula $D_{k+1}$
\end_inset

.
 Therefore, the algoritm on each iteration tries to get a weak classifiers
 minimizing errors of previously selected ones, the empirical error of AdaBoost
 classifier decreases exponentially fast as a function of the number of
 boosting rounds 
\begin_inset Formula $T$
\end_inset

 in formula
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:adaboost_ensemble"
plural "false"
caps "false"
noprefix "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

.
 In this study the separarable datasets are explored, more specificly, the
 training set is expected to be labeled by the final classifier without
 errors, i.e.
 
\begin_inset Formula $y_{t}∙f(x_{t})>0,t=1..m$
\end_inset

.
 For this case we may define 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- geometric
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 margin, which characterises a confidence of classifier 
\begin_inset Formula $f$
\end_inset

 on the training data set:
\begin_inset Formula 
\begin{equation}
ρ_{f}=\underset{t=1..m}{min}|α·h(x_{t})|/\|α\|_{1}\label{eq:ro_f}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In other words, 
\begin_inset Formula $ρ_{f}$
\end_inset

 is a minimum among norm-1 distances from 
\begin_inset Formula $h(x_{t})$
\end_inset

 vectors to the hyperplane of equation 
\begin_inset Formula $α∙x=0$
\end_inset

 in 
\begin_inset Formula $R^{T}$
\end_inset

.
 In the following paragraphs we will explore the geometric margin value
 and its relationship to other parameters in different experiments.
\end_layout

\begin_layout Subsection
\noindent
Linear progrmming algorithm
\end_layout

\begin_layout Standard
\noindent
The 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- geometric
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 margin
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ro_f"
plural "false"
caps "false"
noprefix "false"

\end_inset

 which is obtained as a parameter of the original AdaBoost can be maximised
 explicitly.
 For this, in separable case the linear programming problem can be formulated
 for the 
\begin_inset Formula $ρ$
\end_inset

 maximisation:
\begin_inset Formula 
\begin{align}
\underset{α}{max}ρ\label{eq:max_1}\\
subject\,to: & y_{t}(α·h(x_{t}))/\|α\|_{1}≥ρ,t=1..m\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In this problem vectors 
\begin_inset Formula $α$
\end_inset

 and 
\begin_inset Formula $h(x_{t})$
\end_inset

 must have dimensionality equal to size of set 
\begin_inset Formula $ℋ$
\end_inset

.
 As described in paragraph 2.1, for training set with n features and m samples
 
\begin_inset Formula $ℋ$
\end_inset

 consists of 2
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
n
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
m classifiers.
 Also, in view of formula 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ro_f"
plural "false"
caps "false"
noprefix "false"

\end_inset

 it is reasonable to add condition 
\begin_inset Formula $\|α\|_{1}=1$
\end_inset

 and get rid of the denomenator in the constraint.
 As a variables of the task a scalar value 
\begin_inset Formula $ρ$
\end_inset

 and vector 
\begin_inset Formula $α$
\end_inset

 must be taken.
 Coordinates of 
\begin_inset Formula $α$
\end_inset

 should be non-negative for model unambiguity, because a base classifier
 with a negative coefficient is equivalent to the symmetric classifier with
 the same modulo positive coefficient.
 The parameter 
\begin_inset Formula $ρ$
\end_inset

 is expected to be positive as a result of optimisation in separable case.
\end_layout

\begin_layout Standard
\noindent
For this problem the implementation in Python with library scipy.optimize.linprog
 is used.
 However, this library does not allow variables in right sides of inequalities
 and accepts only minimisation problems.
 Thus, the original problem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:max_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 must be slightly transformed.
 Firstly, a vector of variables must be defined, let's denote it as 
\begin_inset Formula $u$
\end_inset

, with first coordinates coinsiding with 
\begin_inset Formula $α$
\end_inset

 and the latest coordinate equal to 
\begin_inset Formula $ρ$
\end_inset

, 2
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
n
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
m+1 coordinates in total.
 Let's denote 2
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
n
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
·
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
m as N.
 Then, the target function is as follows:
\begin_inset Formula 
\[
\underset{u}{min}-u_{N+1}
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
Next, there are m constraints for each sample from test set:
\begin_inset Formula 
\[
u_{1}(-y_{t}·h_{1}(x_{t}))+...+u_{N}(-y_{t}·h_{N}(x_{t}))+u_{N+1}≤0,t=1..m
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
and constraint 
\begin_inset Formula $\|α\|_{1}=1$
\end_inset

:
\begin_inset Formula 
\[
\sum{}_{k\text{=1}}^{N}u_{k}=1
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
On top of that, bounds for 
\begin_inset Formula $α$
\end_inset

 must be defined:
\begin_inset Formula 
\[
u_{k}≥0,k=1..N
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
To formulate the task, all classifiers from 
\begin_inset Formula $ℋ$
\end_inset

 on all test samples must be calculated.
 In the current example for training set with n features and m samples there
 are 
\begin_inset Formula $N·m=2·n·m^{2}$
\end_inset

 values which is a big value in real experiments.
 Nevertheless, the linear algorithm directly maximises 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- geometric
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 margin for the given hypothesis set 
\begin_inset Formula $ℋ$
\end_inset

 and therefore is of theoretical interest.
\end_layout

\begin_layout Subsection
\noindent
Margin theory
\begin_inset CommandInset label
LatexCommand label
name "subsec:Margin-theory"

\end_inset


\end_layout

\begin_layout Standard
\noindent
In line with statistical learning theory, there are estimations of generalisatio
n error for real-valued hypothesis.
 At the beginning, some notions need to be defined.
 The following definitions are generally accepted in statistic learning
 theory and cited according to the book 
\begin_inset Quotes eld
\end_inset

Foundations of Machine Learning
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

.
\end_layout

\begin_layout Standard
\noindent
Generalisation error: for a given hypothesis 
\begin_inset Formula $h\inℋ$
\end_inset

 , true classifier 
\begin_inset Formula $c$
\end_inset

 and distribution 
\begin_inset Formula $𝒟$
\end_inset

, the generalisation error 
\begin_inset Formula $R$
\end_inset

 is defined as follows:
\begin_inset Formula 
\begin{equation}
R(h)=\underset{x\sim𝒟}{𝕇}[h(x)≠c(x)]=\underset{x\sim𝒟}{𝔼}[1_{h(x)≠c(x)}]\label{eq:gen_err}
\end{equation}

\end_inset

Empirical Rademacher complexity for hypothesis set: Let 
\begin_inset Formula $ℋ$
\end_inset

 be a family of functions taking values in {−1, +1}, then for sample 
\begin_inset Formula $S_{m}$
\end_inset


\begin_inset Formula 
\begin{equation}
\hat{𝔑_{s}}(ℋ)=\frac{1}{m}\underset{σ}{𝔼}[\underset{h\inℋ}{sup}\sum{}_{t\text{=1}}^{m}σ_{t}·h(x_{t})]\label{eq:rad_h}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In the formula above 
\begin_inset Formula $σ=(σ_{1},...,σ_{m})$
\end_inset

, where 
\begin_inset Formula $σ_{t},t=1..m$
\end_inset

 are independent uniform random variables taking values in {−1, +1}.
\end_layout

\begin_layout Standard
\noindent
Empirical margin loss for sample 
\begin_inset Formula $S_{m}$
\end_inset

:
\begin_inset Formula 
\begin{equation}
\hat{R}_{S,ρ}(h)=\frac{1}{m}\sum{}_{t\text{=1}}^{m}𝛷_{ρ}(y_{t}·h(x_{t}))\label{eq:mg_loss}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
In this formula 
\begin_inset Formula $𝛷_{ρ}(x)=min(1,max(0,1-x/ρ))$
\end_inset

, the graph of the function is shown in figure 1.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename pic/loss_function.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Marging loss function
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Finally, the estimate of the upper limit of the overall error is as follows.
 Let 
\begin_inset Formula $ℋ$
\end_inset

 be a family of real-valued functions and the non-negative value 
\begin_inset Formula $ρ$
\end_inset

 of margin is fixed.
 Then, for any 
\begin_inset Formula $δ>0$
\end_inset

, with probability at least 
\begin_inset Formula $1−δ$
\end_inset

, the following holds for all 
\begin_inset Formula $h∈ℋ$
\end_inset

:
\begin_inset Formula 
\begin{equation}
R(h)≤\hat{R}_{S,ρ}(h)+\frac{2}{ρ}\hat{𝔑_{s}}(ℋ)+3\sqrt{\frac{log\frac{1}{δ}}{2m}}\label{eq:est_1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
Remarkably, that this formula remains valid if we consider convex linear
 combinations of classifiers in 
\begin_inset Formula $ℋ$
\end_inset

 , i.
 e.
 if 
\begin_inset Formula $h(x)\in conv(ℋ)$
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

, where 
\begin_inset Formula $conv(ℋ)$
\end_inset

 is:
\begin_inset Formula 
\begin{equation}
conv(ℋ)=\{\sum{}_{t\text{=1}}^{m}µ_{t}h_{t}:m≥1,∀t=1..m,\text{µ}_{t}≥0,h_{t}\inℋ,\sum{}_{t\text{=1}}^{m}\text{µ}_{t}≤1\}\label{eq:conv}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
As AdaBoost classifiers 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:adaboost_ensemble"
plural "false"
caps "false"
noprefix "false"

\end_inset

 of both previously described versions are ensembles satisfying condition
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:conv"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the inequality 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is applicable for their generalisation error estimation.
 Since both versions work with the same hypothesis set 
\begin_inset Formula $ℋ$
\end_inset

 and we may fix 
\begin_inset Formula $δ$
\end_inset

 value for all experiments, the difference between them depends on margin
 loss term 
\begin_inset Formula $\hat{R}_{S,ρ}(h)$
\end_inset

.
 According to the theory, for marging lost calculation we use the final
 classifier's ansemble 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:adaboost_ensemble"
plural "false"
caps "false"
noprefix "false"

\end_inset

 but without an external sign operator.
 Thus, in separable case expression 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mg_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be rewrittern as follows:
\begin_inset Formula 
\begin{equation}
\hat{R}_{S,ρ}(h)=\frac{1}{m}\sum{}_{t\text{=1}}^{m}𝛷_{ρ}(α·h(x_{t}))\label{eq:mg_loss_sep}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\noindent
A comparison of formulae 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ro_f"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mg_loss_sep"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows that for 
\begin_inset Formula $ρ<ρ_{f}$
\end_inset

 
\begin_inset Formula $\hat{R}_{S,ρ}(h)=0$
\end_inset

.
 Obviously, linear programming algorithm yields larger margin, but question
 if this specificity is decisive in estimating 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is debatable and will be addressed in subsequent experiments.
\end_layout

\begin_layout Section
\noindent
Experiments
\end_layout

\begin_layout Standard
\noindent
In the previous paragraphs, several questions have been posed for investigation
 in real-data experiments.
 Firstly, there is a 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- geometric
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 margin 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ro_f"
plural "false"
caps "false"
noprefix "false"

\end_inset

 which reflects the classifier's confidence on train dataset, and question
 is whether greater 
\begin_inset Formula $L_{1}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
- geometric
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 margin provides less generalisation error on test data.
 Secondly, we need to check terms of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

 on real data and estimate gap between the right-hand side of this inequality
 and the errors in the test samples.
\end_layout

\begin_layout Subsection
Estimation of 
\begin_inset Formula $L_{1}$
\end_inset

- geometric margin
\end_layout

\begin_layout Standard
Estimations for geometric margin have been performed on synthetic or generated
 and real data sets, results are represented in 
\begin_inset CommandInset href
LatexCommand href
name "Geometric_Margin_Estimations.ipynb"
target "https://github.com/vkmbox/Ml-hse-final/blob/master/ada-boost-implementations/code-python/notebooks/Geometric_Margin_Estimations.ipynb"
literal "false"

\end_inset

 notebook.
 For data generation the Python function 
\begin_inset Newline newline
\end_inset

sklearn.datasets.make_classification was used with the following default parameter
s: number of samples=300, total number of features=50, number of informative
 features=20, number of redundant features=20, number of duplicated features=10,
 number of clusters per class=5.
 Since we plan to perform experiments on separable data sets, labels for
 samples X are artificially separated.
 For this, a random vector is generated and the labels {-1, +1} are created
 according to the projection of X onto this vector.
 Moreover, samples X are artificially extended from the hyperplane perpendicular
 to the vector by a gap value, the default gap is 0.1.
 Varying the gap we may simulate different kinds of separable or non-separable
 data sets.
 Default splitting of generated data is 
\begin_inset Formula $\frac{2}{3}$
\end_inset

 for train set and 
\begin_inset Formula $\frac{1}{3}$
\end_inset

 for test set.
\end_layout

\begin_layout Subsubsection
Synthetic datasets with a varying number of properties
\end_layout

\begin_layout Standard
\noindent
In the first experiment, there are data sets on 200 rows with the number
 of features changing from 25 to 500 in increments of 25.
 For each number of features 5 data sets are generated and the results are
 averaged.
 Both version of classifiers (linear and AdaBoost) are learned on subset
 of 200 rows and then empirical risk is estimated on 100 rows set.
 The margin value and empirical risk in the experiments is illustrated by
 figure 2.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pic/features_margin.png
	width 50col%

\end_inset


\begin_inset Graphics
	filename pic/features_risk.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Margin and empirical risk depending on features count in experiments with
 300 samples datasets
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
As can be seen from the graphs, geometrical margin for the linear algorithm
 is higher, which is explained by the different principles of the algorithms.
 For AdaBoost the number of non-zero coefficients in the linear combination
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:adaboost_ensemble"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is limited by the quantity of boosting rounds, while in linear algorithm
 version a coefficient of any weak classifier from hypotesis set 
\begin_inset Formula $ℋ$
\end_inset

 can be non-zero and set of possible combinations is obviously larger in
 the latter case.
 At the same time, empirical risk does not differ significantly between
 the versions of the classifiers, as can be seen from the right-hand grap.
\end_layout

\begin_layout Standard
\noindent
Also, in this experiment we can investigate if there is a statically significant
 difference between empirical risks of AdaBoost and linear classifiers.
 For analysis we have two sets of risk values with the corresponding values
 referring to the results of the classifiers on the same dataset.
 It is difficult to determine the type of distribution of risk values, however,
 it can be hypothesised that these values has equal median.
 In other words, the main hypothesis of no difference on median 
\begin_inset Formula $H_{0}$
\end_inset

 can be tested against the alternative hypothesis 
\begin_inset Formula $H_{1}$
\end_inset

 of a statistical significant difference for the two-sided criterion.
 Since the distribution of variables is unknown, we may apply two sample
 non-parametric methods such as the sign
\begin_inset CommandInset citation
LatexCommand cite
key "Sign"
literal "true"

\end_inset

 and Wilcoxon
\begin_inset CommandInset citation
LatexCommand cite
key "Wilcoxon"
literal "true"

\end_inset

 tests.
 For sign test p–value=0.33 and for Wilcoxon test p–value=0.29 
\begin_inset CommandInset href
LatexCommand href
name "(link)"
target "https://github.com/vkmbox/Ml-hse-final/blob/master/ada-boost-implementations/code-python/notebooks/Geometric_Margin_Estimations.ipynb"
literal "false"

\end_inset

 and hypothesis 
\begin_inset Formula $H_{1}$
\end_inset

 is rejected.
 The criterion Mann–Whitney U is not applicable here since error samples
 are dependent.
\end_layout

\begin_layout Subsubsection
Synthetic datasets with a varying number of samples
\end_layout

\begin_layout Standard
\noindent
Second experiment is performed with default number of features=50 and varying
 number of samples from 100 to 500 with step 50.
 Results are on figure 3.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pic/samples_margin.png
	width 50col%

\end_inset


\begin_inset Graphics
	filename pic/samples_risk.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Margin and empirical risk depending on samples number, 50 features
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Again, the significant difference in margin values can be seen, which does
 not, however, affect the empirical risk values.
 In this experiment sign test gives positive criterion value with p–value=0.006,
 which leads one to assume that, according to the way the criterion is calculate
d, hypotesis that empirical risk median for linear algorithm is larger than
 ada-boost's one is true at a significance level of 0.05.
 Wilcoxon test supports the hypothesis of medians unequality with a p-value=0.015.
\end_layout

\begin_layout Subsubsection
Synthetic datasets with a varying percent of informative features
\end_layout

\begin_layout Standard
\noindent
In the next experiment number of samples=300 and features=100 is fixed,
 and the percent of informative features is varying from 10% to 90% with
 the appropriate change of redundant features from 80% to 0%, number of
 repeated features is fixed to 10%.
 Results are shown on figure 4.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename pic/informative_margin.png
	width 50col%

\end_inset


\begin_inset Graphics
	filename pic/informative_risk.png
	width 50col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Margin and empirical risk depending on percent of informative features,
 300 samples and 100 features
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
The ratio of the data is similar to previous experiments, difference in
 margin doesn't lead to better empirical risk.
 For this experiments, p–value are as follows: sign test - 0.26, Wilcoxon
 - 0.49.
 In both tests the hypotesis 
\begin_inset Formula $H_{1}$
\end_inset

 of difference in empirical risk between classifiers is rejected.
\end_layout

\begin_layout Subsubsection
\noindent
Breast cancer dataset
\end_layout

\begin_layout Standard
\noindent
Of particular interest is a study on real data sets.
 Several experiments with real data sets from the UCI repository are described
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Breiman1997"
literal "true"

\end_inset

, here some of these datasets are retested.
 The first date set is Breast cancer
\begin_inset CommandInset citation
LatexCommand cite
key "Breast_Cancer"
literal "true"

\end_inset

, this data set has 9 informative features and 699 rows.
 There are 458 rows for benign and 241 rows for malignant cases, the data
 can be considered broadly balanced.
 To test that the data set is separable, a support vector classifier is
 used.
 After training on the whole dataset, the svm classifier correctly predicts
 class for all samples, i.e.
 it is possible to construct a hyperplane in the feature space separating
 the data.
 The complete data set is splitted 20 times at random into training and
 test samples, with 1/4 of the records for the test, for each split geometric
 margin and empirical risk are calculated, results are in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:breast-cancer"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="right" valignment="top">
<column alignment="right" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter name
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Average value
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Standard deviation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for linear algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0622857143
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0133889994
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for ada boost algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0517142857
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0096425926
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Geometric margin for linear algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0380784851
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0104855946
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Geometric margin for ada boost algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0166040126
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0141669141
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:breast-cancer"

\end_inset

Empirical error and geometric margin in experiments with breast cancer data
 set
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
For two sets of errors, 20 measurements for each classifiers, we apply statistic
 criteria to check significant difference between classifiers similar to
 previous experiments.
 Sign test gives p–value = 0.049 and Wilcoxon - p–value = 0.0046, both tests
 reject hypotesis 
\begin_inset Formula $H_{0}$
\end_inset

 about error median equality at 0.05 significance level.
 Finaly, since the hypotheses of equality are rejected for the two-sided
 criterion and the error results are higher for the linear algorithm, we
 can accept the hypothesis of a larger error for the linear algorithm at
 the 0.05 significance level.
\end_layout

\begin_layout Subsubsection
\noindent
Ionosphere dataset
\end_layout

\begin_layout Standard
\noindent
Second studying real data set is Ionosphere
\begin_inset CommandInset citation
LatexCommand cite
key "Ionosphere"
literal "true"

\end_inset

, there are 351 rows and 34 features there, The set includes 126 rows for
 good and 225 rows for bad cases, data are quite balanced.
 Since the set is relatively small, 50 splitting are performed with 1/4
 of the records for the test with the calculation of geometric margin and
 empirical risk for each splitting.
 Results are collected in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Ionosphere"
plural "false"
caps "false"
noprefix "false"

\end_inset

:
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="left" valignment="top">
<column alignment="right" valignment="top">
<column alignment="right" valignment="top">
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Parameter name
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Average value
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Standard deviation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for linear algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1020454545
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0280651912
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Empirical error for ada boost algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0931818182
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0280200636
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Geometric margin for linear algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1096625957
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0050373506
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Geometric margin for ada boost algorithm
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0961726663
\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0054469719
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Ionosphere"

\end_inset

Empirical error and geometric margin in experiments with breast cancer data
 set
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
Statistical criteria for two sets of errors with 50 measurements for each
 classifiers give p–values as follows: sign test - 6.96e-05, Wilcoxon - pvalue=0.0
002.
 Thus, both criteria reject hypothesis 
\begin_inset Formula $H_{0}$
\end_inset

 about error median equality with high significance.
\end_layout

\begin_layout Subsubsection
\noindent
Conclusions on the dependence of the empirial error from 
\begin_inset Formula $L_{1}$
\end_inset

- geometric margin
\end_layout

\begin_layout Standard
No experiment described above shows a smaller error for a linear algorithm
 classifier in comparison with ada-boost one, though it has always larger
 
\begin_inset Formula $L_{1}$
\end_inset

- geometric margin value on the training set.
 Conversely, in experiments with real datasets we get a statistically significan
t difference in favour of a smaller empirical error in the ada-boost case.
 To summarise, higher margin value on training set doesn't guarantee lower
 error in the considered case of comparison of ada-boost and linear algorithm
 classifiers.
\end_layout

\begin_layout Subsection
Estimation of generalisation error
\end_layout

\begin_layout Standard
This section discusses the empirical error estimates derived from inequality
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:est_1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 For analysis of this inequality, we present it here again:
\begin_inset Formula 
\[
R(h)≤\hat{R}_{S,ρ}(h)+\frac{2}{ρ}\hat{𝔑_{s}}(ℋ)+3\sqrt{\frac{log\frac{1}{δ}}{2m}}
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
The inequality holds for any classifier 
\begin_inset Formula $h\inℋ$
\end_inset

 where 
\begin_inset Formula $ℋ$
\end_inset

 is a family of real-valued functions, with probability at least 
\begin_inset Formula $1−δ$
\end_inset

.
 As it is already discuss in paragraph 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Theoretical-basis"
plural "false"
caps "false"
noprefix "false"

\end_inset

, both ada-boost and linear algorithms work with the same hypothesis set
 
\begin_inset Formula $ℋ$
\end_inset

 and fixing 
\begin_inset Formula $δ=0.05$
\end_inset

 in subsequent experiments we ensure the equality of right-hand second and
 third terms between the algorithms.
 Thus, the difference in calculation between classifiers is only in the
 value of the Emperical margin loss 
\begin_inset Formula $\hat{R}$
\end_inset

, which is evaluated by formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mg_loss_sep"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
\noindent
Rademacher empirical complexity calculation on real data set
\end_layout

\begin_layout Standard
Rademacher empirical complexity is calculated by formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:rad_h"
plural "false"
caps "false"
noprefix "false"

\end_inset

:
\begin_inset Formula 
\begin{equation}
\hat{𝔑_{s}}(ℋ)=\frac{1}{m}\underset{σ}{𝔼}[\underset{h\inℋ}{sup}\sum{}_{t\text{=1}}^{m}σ_{t}·h(x_{t})]\label{eq:rad_empirical_2}
\end{equation}

\end_inset

In this formula 
\begin_inset Formula $σ=(σ_{1},...,σ_{m})$
\end_inset

, where 
\begin_inset Formula $σ_{t},t=1..m$
\end_inset

 are independent uniform random variables taking values in {−1, +1}.
 Full sample space has size 
\begin_inset Formula $2^{m}$
\end_inset

 which is a quite big number on real training sets.
 However the calculation can be performed on a subset of the full space
 with satisfactory accuracy, this can be proved with usage of Hoeffding’s
 inequality
\begin_inset CommandInset citation
LatexCommand cite
key "Mohri2018"
literal "true"

\end_inset

.
\end_layout

\begin_layout Standard
\noindent
Hoeffding’s inequality: Let 
\begin_inset Formula $X_{1},...,X_{n}$
\end_inset

 be independent random variables with 
\begin_inset Formula $X_{k}$
\end_inset

 taking values in 
\begin_inset Formula $[a_{k},b_{k}],k=1...n$
\end_inset

.
 Then, for any 
\begin_inset Formula $ε>0$
\end_inset

, the following inequalities hold for 
\begin_inset Formula $S_{n}=\sum{}_{k\text{=1}}^{n}X_{k}$
\end_inset

:
\begin_inset Formula 
\begin{align}
P[(S−E[S_{n}]≥ε)≤e^{-2ε^{2}/\sum{}_{k=1}^{n}(b_{k}-a_{k})^{2}}\label{eq:hoeffding-ineq}\\
P(S_{n}−E[S_{n}]≤-ε)≤e^{-2ε^{2}/\sum{}_{k=1}^{n}(b_{k}-a_{k})^{2}}\nonumber 
\end{align}

\end_inset

Then, сonsider the space of events 
\begin_inset Formula $Ω=\{σ=(σ_{1},...,σ_{m})\},P(σ)=\frac{1}{2^{m}}$
\end_inset

, and random variable 
\begin_inset Formula $X_{k}(σ)=\underset{h\inℋ}{\frac{1}{m}sup}\sum{}_{t\text{=1}}^{m}σ_{t}·h(x_{t})$
\end_inset

.
 In this case 
\begin_inset Formula $X_{k}(σ)\in[-1,1]$
\end_inset

.
 Also, if 
\begin_inset Formula $σ_{k}$
\end_inset

 is selected from 
\begin_inset Formula $Ω$
\end_inset

 whith returning, then 
\begin_inset Formula $X_{k}=X(σ_{k})$
\end_inset

 are independent for different k since different measurements do not affect
 each other.
 Thus, 
\begin_inset Formula $X_{k}$
\end_inset

 variables are i.i.d.
 Finally, we may use 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hoeffding-ineq"
plural "false"
caps "false"
noprefix "false"

\end_inset

 as follows:
\begin_inset Formula 
\[
P(S_{n}−E[S_{n}]≥nε)=P(S_{n}/n−E[S_{n}]/n≥ε)≤e^{-2ε^{2}n^{2}/4n}=e^{-nε^{2}/2}
\]

\end_inset

Notiice, that due to linearity of the mathematical expectation and by defenition
 of Rademacher empirical complexity 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:rad_empirical_2"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $E[S_{n}]/n=E[X_{k}(σ)]=\hat{𝔑_{s}}(ℋ)$
\end_inset

.
 Considering the symmetry of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hoeffding-ineq"
plural "false"
caps "false"
noprefix "false"

\end_inset

 with respect to 
\begin_inset Formula $E[S_{n}]$
\end_inset

, it can be concluded that
\begin_inset Formula 
\[
P(|S_{n}/n−\hat{𝔑_{s}}(ℋ)|≥ε)≤2e^{-nε^{2}/2}
\]

\end_inset

Setting the level of significance 
\begin_inset Formula $δ$
\end_inset

, we obtain the following estimate of the value 
\begin_inset Formula $Δ=|S_{n}/n−\hat{𝔑_{s}}(ℋ)|$
\end_inset

:
\begin_inset Formula 
\[
2e^{-nε^{2}/2}≤δ;n≥\frac{2}{ε^{2}}ln\frac{2}{δ}
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
For 
\begin_inset Formula $ε=0.1$
\end_inset

 and 
\begin_inset Formula $δ=0.05$
\end_inset

 we obtain 
\begin_inset Formula $n≥737.775$
\end_inset

, thus, substituting 
\begin_inset Formula $\hat{𝔑_{s}}(ℋ)$
\end_inset

 with 
\begin_inset Formula $S_{n}/n$
\end_inset

 on 1000 randomly selected Rademacher vectors should give a deviation within
 
\begin_inset Formula $±0.1$
\end_inset

 with 
\begin_inset Formula $P≥0.95$
\end_inset

.
\end_layout

\begin_layout Section
\noindent
Conclusions
\end_layout

\begin_layout Standard
\noindent
test
\begin_inset Newpage newpage
\end_inset


\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "content"
options "unsrt"

\end_inset


\end_layout

\end_body
\end_document
