The plan of the thesis is to evaluate equations 7.12 and 7.15 of the book "Foundations of machine learning" of Mohri and others on real datasets. The aim is to better understand the theory of Rademacher complexity.

As explained in chapter 7, current explanations for AdaBoost rely on a margin hypothesis. But if the margin hypothesis is optimized directly as in section 7.3.4 on page 161, then the resulting algorithm works more poorly. It is currently still not understood why this is.  This effect was first reported in the attached paper.  The plan is to look at various intermediate equations in the proofs of the bounds 7.12 and 7.15. And hopefully, something could be observed there that gives a hint on why AdaBoost works better.  The direct optimization of certain intermediate quantities is expected to give classifiers with better accuracy, but at a bigger computational cost. The hope is to analyse this (it has never been done as far as I know).